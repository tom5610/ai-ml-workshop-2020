{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AI Services for Analyzing Images and Text\n",
    "by Manav Sehgal | on APR 30 2019 | \n",
    "by Tom Liu | on Dec 2020 | modified edition for AI Labs for Amazon Recognition and Amazon Comprehend\n",
    "\n",
    "So far we have been working with structured data in flat files as our data source. What if the source is images and unstructured text. AWS AI services provide vision, transcription, translation, personalization, and forecasting capabilities without the need for training and deploying machine learning models. AWS manages the machine learning complexity, you just focus on the problem at hand and send required inputs for analysis and receive output from these services within your applications.\n",
    "\n",
    "Extending our open data analytics use case to New York Traffic let us use the AWS AI services to turn open data available in social media, Wikipedia, and other sources into structured datasets and insights.\n",
    "\n",
    "We will start by importing dependencies for AWS SDK, Python Data Frames, file operations, handeling JSON data, and display formatting. We will initialize the Rekognition client for use in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "from IPython.display import display, Markdown, Image\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "rekognition = boto3.client('rekognition', region)\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "prefix = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download image set for the lab\n",
    "!wget https://df4l9poikws9t.cloudfront.net/images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -d test_images images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./test_images s3://$bucket_name/$prefix/  --recursive --include \"*.png\" --include \"*.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Image\n",
    "We will work with a number of images so we need a way to show these images within this notebook. Our function creates a public image URL based on S3 bucket and key as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(filename, img_width = 300):\n",
    "    return Image(filename = filename, width = img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'sydney-street-02-unsplash.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_image(f'./test_images/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Labels\n",
    "One of use cases for traffic analytics is processing traffic CCTV imagery or social media uploads. Let's consider a traffic location where depending on number of cars, trucks, and pedestrians we can identify if there is a traffic jam. This insight can be used to better manage flow of traffic around the location and plan ahead for future use of this route.\n",
    "\n",
    "First step in this kind of analytics is to recognize that we are actually looking at an image which may represent a traffic jam. We create ``image_labels`` function which uses ``detect_lables`` Rekognition API to detect objects within an image. The function prints labels detected with confidence score.\n",
    "\n",
    "In the given example notice somewhere in the middle of the labels listing at 73% confidence the Rekognition computer vision model has actually determined a traffic jam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_labels(bucket, key):\n",
    "    image_object = {'S3Object':{'Bucket': bucket,'Name': key}}\n",
    "\n",
    "    response = rekognition.detect_labels(Image=image_object)\n",
    "    for label in response['Labels']:\n",
    "        print('{} ({:.0f}%)'.format(label['Name'], label['Confidence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels(bucket_name, f'images/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "* How well image label detection works for images, such as 'olive_*.png', 'gear*.png' & 'coffee*.png'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Label Count\n",
    "Now that we have a label detecting a traffic jam and some of the ingredients of a busy traffic location like pedestrians, trucks, cars, let us determine quantitative data for benchmarking different traffic locations. If we can count the number of cars, trucks, and persons in the image we can compare these numbers with other images. Our function does just that, it counts the number of instances of a matching label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_label_count(bucket, key, match):    \n",
    "    image_object = {'S3Object':{'Bucket': bucket,'Name': key}}\n",
    "\n",
    "    response = rekognition.detect_labels(Image=image_object)\n",
    "    count = 0\n",
    "    for label in response['Labels']:\n",
    "        if match in label['Name']:\n",
    "            for instance in label['Instances']:\n",
    "                count += 1\n",
    "    print(f'Found {match} {count} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_count(bucket_name, f'images/{file_name}', 'Car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_count(bucket_name, f'images/{file_name}', 'Truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_count(bucket_name, f'images/{file_name}', 'Person')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Text\n",
    "Another use case of traffic location analytics using social media content is to understand more about a traffic location and instance if there is an incident reported, like an accident, jam, or VIP movement. For a computer program to understand a random traffic location, it may help to capture any text within the image. The ``image_text`` function uses Amazon Rekognition service to detect text in an image.\n",
    "\n",
    "You will notice that the text recognition is capable to read blurry text like \"The Lion King\", text which is at a perspective like the bus route, text which may be ignored by the human eye like the address below the shoes banner, and even the text representing the taxi number. Suddenly the image starts telling a story programmatically, about what time it may represent, what are the landmarks, which bus route, which taxi number was on streets, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text(bucket, key, sort_column='', parents=True):\n",
    "    response = rekognition.detect_text(Image={'S3Object':{'Bucket':bucket,'Name': key}})\n",
    "    df = pd.read_json(io.StringIO(json.dumps(response['TextDetections'])))\n",
    "    df['Width'] = df['Geometry'].apply(lambda x: x['BoundingBox']['Width'])\n",
    "    df['Height'] = df['Geometry'].apply(lambda x: x['BoundingBox']['Height'])\n",
    "    df['Left'] = df['Geometry'].apply(lambda x: x['BoundingBox']['Left'])\n",
    "    df['Top'] = df['Geometry'].apply(lambda x: x['BoundingBox']['Top'])\n",
    "    df = df.drop(columns=['Geometry'])\n",
    "    if sort_column:\n",
    "        df = df.sort_values([sort_column])\n",
    "    if not parents:\n",
    "        df = df[df['ParentId'] > 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_image_file = 'street-01-unsplash.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(f'./test_images/{text_image_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting on ``Top`` column will keep the horizontal text together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_text(image_bucket, f'images/{text_image_file}', sort_column='Top', parents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "* How well the image text detection function works for images, such as 'olive_coffee_shop_*.png'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Celebs\n",
    "Traffic analytics may also involve detecting VIP movement to divert traffic or monitor security events. Detecting VIP in a scene starts with facial recognition. Our function ``detect_celebs`` works as well with political figures as it will with movie celebrities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_celebs(bucket, key, sort_column=''):\n",
    "    image_object = {'S3Object':{'Bucket': bucket,'Name': key}}\n",
    "\n",
    "    response = rekognition.recognize_celebrities(Image=image_object)\n",
    "    df = pd.DataFrame(response['CelebrityFaces'])\n",
    "    df['Width'] = df['Face'].apply(lambda x: x['BoundingBox']['Width'])\n",
    "    df['Height'] = df['Face'].apply(lambda x: x['BoundingBox']['Height'])\n",
    "    df['Left'] = df['Face'].apply(lambda x: x['BoundingBox']['Left'])\n",
    "    df['Top'] = df['Face'].apply(lambda x: x['BoundingBox']['Top'])\n",
    "    df = df.drop(columns=['Face'])\n",
    "    if sort_column:\n",
    "        df = df.sort_values([sort_column])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image('./test_images/celeb-02-unsplash.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_celebs(image_bucket, 'images/celeb-02-unsplash.jpg', sort_column='Left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehend Syntax\n",
    "It is possible that many data sources represent natural language and free text. Understand structure and semantics from this unstructured text can help further our open data analytics use cases.\n",
    "\n",
    "Let us assume we are processing traffic updates for structured data so we can take appropriate actions. First step in understanding natural language is to break it up into grammaticaly syntax. Nouns like \"today\" can tell about a particular event like when is the event occuring. Adjectives like \"snowing\" and \"windy\" tell what is happening at that moment in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client('comprehend', region)\n",
    "\n",
    "traffic_update = \"\"\"\n",
    "It is snowing and windy today in New York. The temperature is 50 degrees Fahrenheit. \n",
    "The traffic is slow 10 mph with several jams along the I-86.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehend_syntax(text): \n",
    "    response = comprehend.detect_syntax(Text=text, LanguageCode='en')\n",
    "    df = pd.read_json(io.StringIO(json.dumps(response['SyntaxTokens'])))\n",
    "    df['Tag'] = df['PartOfSpeech'].apply(lambda x: x['Tag'])\n",
    "    df['Score'] = df['PartOfSpeech'].apply(lambda x: x['Score'])\n",
    "    df = df.drop(columns=['PartOfSpeech'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_syntax(traffic_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehend Entities\n",
    "More insights can be derived by doing entity extraction from the natural langauage. These entities can be date, location, quantity, among others. Just few of the entities can tell a structured story to a program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehend_entities(text):\n",
    "    response = comprehend.detect_entities(Text=text, LanguageCode='en')\n",
    "    df = pd.read_json(io.StringIO(json.dumps(response['Entities'])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_entities(traffic_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehend Phrases\n",
    "Analysis of phrases within narutal language text complements the other two methods for a program to better route the actions based on derived structure of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehend_phrases(text):\n",
    "    response = comprehend.detect_key_phrases(Text=text, LanguageCode='en')\n",
    "    df = pd.read_json(io.StringIO(json.dumps(response['KeyPhrases'])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_phrases(traffic_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehend Sentiment\n",
    "Sentiment analysis is common for social media user generated content. Sentiment can give us signals on the users' mood when publishing such social data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehend_sentiment(text):\n",
    "    response = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "    return response['SentimentScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_sentiment(traffic_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your thoughts and check the related sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_sentiment(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original notebook\n",
    "\n",
    "[Original notebook](https://github.com/aws-samples/aws-open-data-analytics-notebooks/blob/master/ai-services/using-ai-services-for-analyzing-public-data.ipynb) created by Manva Sehgal on APR 30 2019\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
